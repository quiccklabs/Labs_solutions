{
  "cells": [
    {
      "cell_type": "code",
      "id": "qD6skGFOVmP1UeWhmH9PHgDb",
      "metadata": {
        "tags": [],
        "id": "qD6skGFOVmP1UeWhmH9PHgDb"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import aiplatform\n",
        "import bigframes.pandas as bpd\n",
        "import pandas as pd\n",
        "from vertexai.language_models._language_models import TextGenerationModel\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from bigframes.ml.cluster import KMeans\n",
        "from bigframes.ml.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'qwiklabs-gcp-03-30f9ddcee663'\n",
        "dataset_name = \"ecommerce\"\n",
        "model_name = \"customer_segmentation_model\"\n",
        "table_name = \"customer_stats\"\n",
        "location = \"us-central1\"\n",
        "client = bigquery.Client(project=project_id)\n",
        "aiplatform.init(project=project_id, location=location)"
      ],
      "metadata": {
        "id": "UJwz-GCO0z7t"
      },
      "id": "UJwz-GCO0z7t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "CREATE OR REPLACE TABLE ecommerce.customer_stats AS\n",
        "SELECT\n",
        "  user_id,\n",
        "  DATE_DIFF(CURRENT_DATE(), CAST(MAX(order_created_date) AS DATE), day) AS days_since_last_order, ---RECENCY\n",
        "  COUNT(order_id) AS count_orders, --FREQUENCY\n",
        "  AVG(sale_price) AS average_spend --MONETARY\n",
        "  FROM (\n",
        "      SELECT\n",
        "        user_id,\n",
        "        order_id,\n",
        "        sale_price,\n",
        "        created_at AS order_created_date\n",
        "        FROM `bigquery-public-data.thelook_ecommerce.order_items`\n",
        "        WHERE\n",
        "        created_at\n",
        "            BETWEEN '2022-01-01' AND '2023-01-01'\n",
        "  )\n",
        "GROUP BY user_id;"
      ],
      "metadata": {
        "id": "6RE-2de01vei"
      },
      "id": "6RE-2de01vei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Convert the table ecommerce.customer_stats to a bigframes dataframe and show the top 10 records\n",
        "\n",
        "# Read the BigQuery table \"ecommerce.customer_stats\" into a BigFrames DataFrame.\n",
        "df = bpd.read_gbq(\"qwiklabs-gcp-03-30f9ddcee663.ecommerce.customer_stats\")\n",
        "\n",
        "# Display the first 10 rows of the DataFrame.\n",
        "result = df.head(10)"
      ],
      "metadata": {
        "id": "mRig8Hpi2PdM"
      },
      "id": "mRig8Hpi2PdM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 1. Split the DataFrame df into training and testing sets using train_test_split with a test size of 0.2 and random_state=42 for reproducibility. Store the splits as df_train and df_test. 2. Create a KMeans clustering model using bigframes.ml.cluster.KMeans with 5 clusters (note: do not include random_state since it's unsupported). 3. Fit the KMeans model on the training data (df_train). 4. Save the trained model to BigQuery using the to_gbq method, specifying the model path as project_id.dataset_name.model_name. 5. Store the trained model object in a variable named result.\n",
        "\n",
        "# Split the DataFrame df into training and testing sets.\n",
        "# We use train_test_split from bigframes.ml.model_selection, specifying the test_size and random_state for reproducibility.\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a KMeans clustering model.\n",
        "# We instantiate KMeans from bigframes.ml.cluster, setting the number of clusters to 5.\n",
        "kmeans_model = KMeans(n_clusters=5)\n",
        "\n",
        "# Fit the KMeans model on the training data.\n",
        "# The fit method trains the model using the features in df_train.\n",
        "kmeans_model.fit(df_train)\n",
        "\n",
        "# Save the trained model to BigQuery.\n",
        "# The to_gbq method saves the model to the specified BigQuery path.\n",
        "kmeans_model.to_gbq(f\"{project_id}.{dataset_name}.{model_name}\")\n",
        "\n",
        "# Store the trained model object in the 'result' variable.\n",
        "result = kmeans_model"
      ],
      "metadata": {
        "id": "gVe9iFQS3TzJ"
      },
      "id": "gVe9iFQS3TzJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: #prompt: 1. Split df (using random state and test size 0.2) into test and training data for a K-means clustering algorithm store these as df_test and df_train. 2. Create a K-means cluster model using bigframes.ml.cluster KMeans with 5 clusters. 3. Save the model using the to_gbq method where the model name is project_id.dataset_name.model_name.\n",
        "# df_train, df_test = train_test_split(bq_df, test_size=0.2, random_state = 42)\n",
        "# kmeans = KMeans(n_clusters=5)\n",
        "# kmeans.fit(df_train)\n",
        "# kmeans.to_gbq(f\"{project_id}.{dataset_name}.{model_name}\")\n",
        "\n",
        "# Split the DataFrame df into training and testing sets.\n",
        "# We use train_test_split from bigframes.ml.model_selection, specifying the test_size and random_state for reproducibility.\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a KMeans clustering model.\n",
        "# We instantiate KMeans from bigframes.ml.cluster, setting the number of clusters to 5.\n",
        "kmeans_model = KMeans(n_clusters=5)\n",
        "\n",
        "# Fit the KMeans model on the training data.\n",
        "# The fit method trains the model using the features in df_train.\n",
        "kmeans_model.fit(df_train)\n",
        "\n",
        "# Save the trained model to BigQuery.\n",
        "# The to_gbq method saves the model to the specified BigQuery path.\n",
        "kmeans_model.to_gbq(f\"{project_id}.{dataset_name}.{model_name}\")\n",
        "\n",
        "# Store the trained model object in the 'result' variable.\n",
        "result = kmeans_model"
      ],
      "metadata": {
        "id": "V_BM8GVV3Y0O"
      },
      "id": "V_BM8GVV3Y0O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 1. Call the K-means prediction model on the df dataframe, and store the results as predictions_df and show the first 10 records.\n",
        "\n",
        "predictions_df = kmeans_model.predict(df)\n",
        "result = predictions_df.head(10)"
      ],
      "metadata": {
        "id": "DcZbQoqG3twg"
      },
      "id": "DcZbQoqG3twg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # prompt: 1. Call the K-means prediction model on the df dataframe, and store the results as predictions_df and show the first 10 records.\n",
        "# predictions_df = kmeans.predict(df_test)\n",
        "# predictions_df.head(10)\n",
        "\n",
        "predictions_df = kmeans_model.predict(df)\n",
        "result = predictions_df.head(10)"
      ],
      "metadata": {
        "id": "BF_6bGAe3xjz"
      },
      "id": "BF_6bGAe3xjz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 1. Using predictions_df, and matplotlib, generate a scatterplot. 2. On the x-axis of the scatterplot, display days_since_last_order and on the y-axis, display average_spend from predictions_df. 3. Color by cluster. 4. The chart should be titled \"Attribute grouped by K-means cluster.\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the scatterplot\n",
        "plt.figure(figsize=(10, 7))\n",
        "scatter = plt.scatter(\n",
        "    predictions_df['days_since_last_order'],\n",
        "    predictions_df['average_spend'],\n",
        "    c=predictions_df['CENTROID_ID'],  # Color by cluster\n",
        "    cmap='viridis',  # Colormap for clusters\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Days Since Last Order')\n",
        "plt.ylabel('Average Spend')\n",
        "plt.title('Attribute grouped by K-means cluster')\n",
        "\n",
        "# Add a colorbar for the clusters\n",
        "plt.colorbar(scatter, label='Cluster ID')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "evRat-xN6Uwe"
      },
      "id": "evRat-xN6Uwe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: #prompt: 1. Using predictions_df, and matplotlib, generate a scatterplot. 2. On the x-axis of the scatterplot, display days_since_last_order and on the y-axis, display average_spend from predictions_df. 3. Color by cluster. 4. The chart should be titled \"Attribute grouped by K-means cluster.\"\n",
        "# import matplotlib.pyplot as plt\n",
        "# # Create the scatter plot\n",
        "# plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "# plt.scatter(predictions_df['days_since_last_order'], predictions_df['average_spend'], c=predictions_df['cluster'], cmap='viridis')\n",
        "# # Customize the plot\n",
        "# plt.title('Attribute grouped by K-means cluster')\n",
        "# plt.xlabel('Days Since Last Order')\n",
        "# plt.ylabel('Average Spend')\n",
        "# plt.colorbar(label='Cluster ID')\n",
        "# # Display the plot\n",
        "# plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the scatterplot\n",
        "plt.figure(figsize=(10, 7))\n",
        "scatter = plt.scatter(\n",
        "    predictions_df['days_since_last_order'],\n",
        "    predictions_df['average_spend'],\n",
        "    c=predictions_df['CENTROID_ID'],  # Color by cluster\n",
        "    cmap='viridis',  # Colormap for clusters\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Days Since Last Order')\n",
        "plt.ylabel('Average Spend')\n",
        "plt.title('Attribute grouped by K-means cluster')\n",
        "\n",
        "# Add a colorbar for the clusters\n",
        "plt.colorbar(scatter, label='Cluster ID')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BoYcvtMb6Z7s"
      },
      "id": "BoYcvtMb6Z7s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "SELECT\n",
        " CONCAT('cluster ', CAST(centroid_id as STRING)) as centroid,\n",
        " average_spend,\n",
        " count_orders,\n",
        " days_since_last_order\n",
        "FROM (\n",
        " SELECT centroid_id, feature, ROUND(numerical_value, 2) as value\n",
        " FROM ML.CENTROIDS(MODEL `{0}.{1}`)\n",
        ")\n",
        "PIVOT (\n",
        " SUM(value)\n",
        " FOR feature IN ('average_spend',  'count_orders', 'days_since_last_order')\n",
        ")\n",
        "ORDER BY centroid_id\n",
        "\"\"\".format(dataset_name, model_name)\n",
        "\n",
        "df_centroid = client.query(query).to_dataframe()\n",
        "df_centroid.head()"
      ],
      "metadata": {
        "id": "DpFEol4N6jWd"
      },
      "id": "DpFEol4N6jWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_query = client.query(query).to_dataframe()\n",
        "df_query.to_string(header=False, index=False)\n",
        "\n",
        "cluster_info = []\n",
        "for i, row in df_query.iterrows():\n",
        " cluster_info.append(\"{0}, average spend ${2}, count of orders per person {1}, days since last order {3}\"\n",
        "  .format(row[\"centroid\"], row[\"count_orders\"], row[\"average_spend\"], row[\"days_since_last_order\"]) )\n",
        "\n",
        "cluster_info = (str.join(\"\\n\", cluster_info))\n",
        "print(cluster_info)"
      ],
      "metadata": {
        "id": "ZYh6FP9h6ltB"
      },
      "id": "ZYh6FP9h6ltB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You're a creative brand strategist, given the following clusters, come up with \\\n",
        "creative brand persona, a catchy title, and next marketing action, \\\n",
        "explained step by step. Identify the cluster number, the title of the person, a persona for them and the next marketing step.\n",
        "\n",
        "Clusters:\n",
        "{cluster_info}\n",
        "\n",
        "For each Cluster:\n",
        "* Title:\n",
        "* Persona:\n",
        "* Next marketing step:\n",
        "\"\"\"\n",
        "\n",
        "responses = model.generate_content(\n",
        "   prompt,\n",
        "   generation_config={\n",
        "      \"temperature\": 0.1,\n",
        "      \"max_output_tokens\": 4000,\n",
        "      \"top_p\": 1.0,\n",
        "      \"top_k\": 40,\n",
        "   }\n",
        ")\n",
        "\n",
        "print(responses.text)"
      ],
      "metadata": {
        "id": "yUwDMBTX7UDK"
      },
      "id": "yUwDMBTX7UDK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: **Cluster 1:**\n",
        "# * **Title:** The Lapsed Loyalists\n",
        "# * **Persona:** These customers have made a purchase in the past but haven't returned for an extended period. They likely had a positive experience but haven't been engaged recently.\n",
        "# * **Next Marketing Step:**\n",
        "#    1. **Re-engagement campaign:** Send personalized emails or targeted ads reminding them of their previous purchase and highlighting new products or promotions that might interest them.\n",
        "#    2. **Offer exclusive discounts or incentives:** Motivate them to return with special offers or loyalty rewards.\n",
        "#    3. **Personalized product recommendations:** Leverage purchase history and browsing behavior to suggest relevant products they might be interested in.\n",
        "# **Cluster 2:**\n",
        "# * **Title:** The Occasional Treaters\n",
        "# * **Persona:** These customers make infrequent purchases but spend more when they do. They likely view the brand as a premium option for special occasions.\n",
        "# * **Next Marketing Step:**\n",
        "#    1. **Highlight exclusivity and premium value:** Emphasize the unique features and benefits of your products to justify the higher price point.\n",
        "#    2. **Offer limited-time promotions or bundles:** Encourage larger purchases with special deals on high-value items or curated product sets.\n",
        "#    3. **Create a sense of urgency and scarcity:** Promote limited-edition products or flash sales to encourage immediate action.\n",
        "# **Cluster 3:**\n",
        "# * **Title:** The One-and-Done Buyers\n",
        "# * **Persona:** These customers have only made a single purchase and haven't returned. They might have had a neutral experience or haven't found a reason to come back.\n",
        "# * **Next Marketing Step:**\n",
        "#    1. **Gather feedback:** Send post-purchase surveys to understand their experience and identify areas for improvement.\n",
        "#    2. **Offer personalized recommendations:** Based on their initial purchase, suggest complementary products or accessories to encourage further engagement.\n",
        "#    3. **Showcase customer testimonials and social proof:** Highlight positive reviews and user-generat\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame from the provided data\n",
        "data = {\n",
        "    'centroid': ['cluster 0', 'cluster 1', 'cluster 2', 'cluster 3', 'cluster 4'],\n",
        "    'average_spend': [100.00, 50.00, 200.00, 75.00, 150.00],\n",
        "    'count_orders': [5, 2, 10, 3, 7],\n",
        "    'days_since_last_order': [30, 90, 10, 60, 20]\n",
        "}\n",
        "df_query = pd.DataFrame(data)\n",
        "\n",
        "# Filter for Cluster 1\n",
        "cluster_1_data = df_query[df_query['centroid'] == 'cluster 1']\n",
        "\n",
        "# Extract the relevant values for Cluster 1\n",
        "cluster_1_average_spend = cluster_1_data['average_spend'].iloc[0]\n",
        "cluster_1_count_orders = cluster_1_data['count_orders'].iloc[0]\n",
        "cluster_1_days_since_last_order = cluster_1_data['days_since_last_order'].iloc[0]\n",
        "\n",
        "# Print the extracted values\n",
        "print(f\"Cluster 1 - Average Spend: ${cluster_1_average_spend:.2f}\")\n",
        "print(f\"Cluster 1 - Count Orders: {cluster_1_count_orders}\")\n",
        "print(f\"Cluster 1 - Days Since Last Order: {cluster_1_days_since_last_order}\")"
      ],
      "metadata": {
        "id": "m7pXWke-7qI5"
      },
      "id": "m7pXWke-7qI5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}