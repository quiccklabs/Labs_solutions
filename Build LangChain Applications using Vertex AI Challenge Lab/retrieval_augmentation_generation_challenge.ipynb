{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9974ac24-0b81-43eb-a8b3-5bc86d402552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip3 install -q google-cloud-aiplatform\n",
    "!pip3 install -q langchain\n",
    "!pip3 install -q langchain-google-genai\n",
    "!pip3 install -q langchain-google-vertexai\n",
    "!pip3 install -q wikipedia\n",
    "!pip3 install -q chromadb\n",
    "!pip3 install -q langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c92d0b4-d250-4a29-ae4b-5f296f14e896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart the kernet after libraries are loaded\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68059ac8-7ae1-4e87-8460-55ec0796ba01",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1607b448-c577-49b6-bf19-41856955b91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "key_name = !gcloud services api-keys list --filter=\"gemini-api-key\" --format=\"value(name)\"\n",
    "key_name = key_name[0]\n",
    "\n",
    "api_key = !gcloud services api-keys get-key-string $key_name --location=\"us-central1\" --format=\"value(keyString)\"\n",
    "api_key = api_key[0]\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b54483-cba4-4191-a3d3-841113acc3fa",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672edc04-3d94-4d8c-8eda-05f0577dd70f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da463ab-5ee2-43cb-a9f0-0c6b0fca00a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your project ID is: qwiklabs-gcp-03-6d5b96fa3458\n"
     ]
    }
   ],
   "source": [
    "# Define project information\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "PROJECT_ID = subprocess.check_output([\"gcloud\", \"config\", \"get-value\", \"project\"], text=True).strip()\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef538a11-c2bd-467d-84d8-453b7243dff7",
   "metadata": {},
   "source": [
    "## Task 1. Load `Documents` from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8eb2ff-bfee-488e-8d38-f7cf68ea9b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the LangChain documentation to load documents for the query below\n",
    "# Set the following parameters:\n",
    "#  * query: \"Gemini GPT-4\"\n",
    "#  * load_max_docs: 10\n",
    "# https://python.langchain.com/docs/integrations/document_loaders/wikipedia\n",
    "\n",
    "query=\"Gemini GPT-4\"\n",
    "max_docs=10\n",
    " \n",
    "docs = WikipediaLoader(query=query, load_max_docs=max_docs).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2608f4d6-4572-4600-906e-b7aed7f9e5e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2. Use `RecursiveTextSplitter` to split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0edc5f5f-0274-4239-b9b0-7c1855011426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents = 10\n"
     ]
    }
   ],
   "source": [
    "# Use the LangChain documentation to split the docs loaded into smaller chunks for indexing\n",
    "# https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"# of documents = {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262239a-a798-4417-aaa2-7c1536d7c1e2",
   "metadata": {},
   "source": [
    "## Task 3. Index Documents in Chroma DB Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c09a444-42c2-42fa-9c85-73bde82a346a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert the correct model name in the constructor below\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#models\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2ef3f05-83eb-4fb3-b864-aec44e6dd257",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gapic client context issue detected.This can occur due to parallelization.\n"
     ]
    }
   ],
   "source": [
    "# Reference the correct parameters (already defined) to properly index \n",
    "# the documents loaded from Wikipedia into Chroma DB as embeddings\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "documents=docs,                 # Data\n",
    "embedding=embeddings,           # Embedding model\n",
    "persist_directory=\"./chroma_db\" # Directory to save data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ee92709-22b6-435c-930f-c33dc309b700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore_disk = Chroma(\n",
    "    persist_directory=\"./chroma_db\", # Directory of db\n",
    "    embedding_function=embeddings    # Embedding model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa643da-2f16-4914-a150-a6045c55d866",
   "metadata": {},
   "source": [
    "## Task 4. Setup a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40965d2b-1858-4189-a553-2baab720d420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup Chroma DB as a `Retriever` for querying the documents\n",
    "# set the k value to 10\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/chroma#retriever-options\n",
    "\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbef6a2d-68a6-41d6-8005-e3fbfd48c1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Gemini_(language_model)', 'summary': \"Google Gemini is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI's GPT-4. It powers the chatbot of the same name.\\n\\n\", 'title': 'Gemini (language model)'}, page_content='Google Gemini is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI\\'s GPT-4. It powers the chatbot of the same name.\\n\\n\\n== History ==\\n\\n\\n=== Development ===\\n\\nGoogle announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor to PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages. Unlike other LLMs, Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code. It had been developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month. In an interview with Wired, DeepMind CEO Demis Hassabis touted Gemini\\'s advanced capabilities, which he believed would allow the algorithm to trump OpenAI\\'s ChatGPT, which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind\\'s AlphaGo program, which gained worldwide attention in 2016 when it defeated Go champion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.\\nIn August 2023, The Information published a report outlining Google\\'s roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard, Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini, along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a \"core contributor\" to Gemini. Because Gemini was being trained on transcripts of YouTube videos, lawyers were brought in to filter out any potentially copyrighted materials.\\nWith news of Gemini\\'s impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to \"an early version\" of the LLM, which Google intended to make available to clients through Google Cloud\\'s Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft\\'s GitHub Copilot.\\n\\n\\n=== Launch ===\\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference. It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, designed for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2. It was made available only in English. Touted as Google\\'s \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google\\'s Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Gemini_(chatbot)', 'summary': 'Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name and developed as a direct response to the meteoric rise of OpenAI\\'s ChatGPT, it was launched in a limited capacity in March 2023 before expanding to other countries in May. It was previously based on PaLM, and initially the LaMDA family of large language models.\\nLaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI\\'s launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard and sent them into a panic, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in February 2023, which took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. Bard and Duet AI were unified under the Gemini brand in February 2024, coinciding with the launch of an Android app, which would replace Google Assistant as the main virtual assistant on Android, Google Assistant, however, will stay as an optional assistant.\\nGemini has received lukewarm responses. It became the center of controversy in February 2024, when social media users reported that it was generating historically inaccurate images of historical figures as people of color, with conservative commentators decrying its alleged bias as \"wokeness\".\\n\\n', 'title': 'Gemini (chatbot)'}, page_content='Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name and developed as a direct response to the meteoric rise of OpenAI\\'s ChatGPT, it was launched in a limited capacity in March 2023 before expanding to other countries in May. It was previously based on PaLM, and initially the LaMDA family of large language models.\\nLaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI\\'s launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard and sent them into a panic, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in February 2023, which took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. Bard and Duet AI were unified under the Gemini brand in February 2024, coinciding with the launch of an Android app, which would replace Google Assistant as the main virtual assistant on Android, Google Assistant, however, will stay as an optional assistant.\\nGemini has received lukewarm responses. It became the center of controversy in February 2024, when social media users reported that it was generating historically inaccurate images of historical figures as people of color, with conservative commentators decrying its alleged bias as \"wokeness\".\\n\\n\\n== Background ==\\n\\nIn November 2022, OpenAI launched ChatGPT, a chatbot based on the GPT-3 family of large language models (LLMs). ChatGPT gained worldwide attention following its release, becoming a viral Internet sensation. Alarmed by ChatGPT\\'s potential threat to Google Search, Google executives issued a \"code red\" alert, reassigning several teams to assist in the company\\'s artificial intelligence (AI) efforts. Sundar Pichai, the CEO of Google and parent company Alphabet, was widely reported to have issued the alert, but Pichai later denied this to The New York Times. In a rare move, Google co-founders Larry Page and Sergey Brin, who had stepped down from their roles as co-CEOs of Alphabet in 2019, were summoned to emergency meetings with company executives to discuss Google\\'s response to ChatGPT. Brin requested access to Google\\'s code in February 2023, for the first time in years.\\nEarlier in 2021, the company had unveiled LaMDA, a prototype LLM, but did not release it to the public. When asked by employees at an all-hands meeting whether LaMDA was a missed opportunity for Google to compete with ChatGPT, Pichai and Google AI chief Jeff Dean stated that while the company had similar capabilities to ChatGPT, moving too quickly in that arena would represent a major \"reputational risk\" due to Google being substantially larger than OpenAI. In January 2023, Google sister company DeepMind CEO Demis Hassabis hinted at plans for a ChatGPT rival, and Google employees were instructed to accelerate progress on a ChatGPT competitor, intensively testing \"Apprentice Bard\" and other chatbots. Pichai assured investors during Google\\'s quarterly earnings investor call in February that the company had plans to expand LaMDA\\'s availability and applications.\\n\\n\\n== History ==\\n\\n\\n=== Announcement ===\\nOn February 6, 2023, Google announced Bard, a generative artificial intelligence chatbot powered by LaMDA. Bard was first rolled out to a select group of 10,000 \"trusted testers\", before a wide release scheduled at the end of the month. The project was overseen by product lead Jack Krawczyk, who described the product as a \"collaborative AI service\" rather than a search engine, while Pichai detailed how Bard would be integrated into Google Search. Reuters calculated that adding ChatGPT-like features to Google Search could cost the company $6 billion in additional expenses by 2024, while research and consulting firm SemiAnalysis calculated that it would cost Google $3 billion. The technology was developed under'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Large_language_model', 'summary': \"A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. Based on language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.\\nLLMs are artificial neural networks that utilize the transformer architecture, invented in 2017. The largest and most capable LLMs, as of June 2024, are built with a decoder-only transformer-based architecture, which enables efficient processing and generation of large-scale text data.\\nHistorically, up to 2020, fine-tuning was the primary method used to adapt a model for specific tasks. However, larger models such as GPT-3 have demonstrated the ability to achieve similar results through prompt engineering, which involves crafting specific input prompts to guide the model's responses. These models acquire knowledge about syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\\nSome notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google's Gemini (the latter of which is currently used in the chatbot of the same name), Meta's LLaMA family of models, Anthropic's Claude models, and Mistral AI's models.\\n\\n\", 'title': 'Large language model'}, page_content='A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. Based on language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.\\nLLMs are artificial neural networks that utilize the transformer architecture, invented in 2017. The largest and most capable LLMs, as of June 2024, are built with a decoder-only transformer-based architecture, which enables efficient processing and generation of large-scale text data.\\nHistorically, up to 2020, fine-tuning was the primary method used to adapt a model for specific tasks. However, larger models such as GPT-3 have demonstrated the ability to achieve similar results through prompt engineering, which involves crafting specific input prompts to guide the model\\'s responses. These models acquire knowledge about syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\\nSome notable LLMs are OpenAI\\'s GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google\\'s Gemini (the latter of which is currently used in the chatbot of the same name), Meta\\'s LLaMA family of models, Anthropic\\'s Claude models, and Mistral AI\\'s models.\\n\\n\\n== History ==\\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achivede then-SOTA perplexity. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.\\n\\nAfter neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before Transformers, it was done by seq2seq deep LSTM networks.\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper\\'s goal was to improve upon 2014 Seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model.\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4.\\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\\'s models'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Microsoft_Copilot', 'summary': 'Microsoft Copilot is a generative artificial intelligence chatbot developed by Microsoft. Based on a large language model, it was launched in February 2023 as Microsoft\\'s primary replacement for the discontinued Cortana.\\nThe service was introduced under the name Bing Chat, as a built-in feature for Microsoft Bing and Microsoft Edge. Over the course of 2023, Microsoft began to unify the Copilot branding across its various chatbot products, cementing the copilot analogy. At its Build 2023 conference, Microsoft announced its plans to integrate Copilot into Windows 11, allowing users to access it directly through the taskbar. In January 2024, a dedicated Copilot key was announced for Windows keyboards.\\nCopilot utilizes the Microsoft Prometheus model, built upon OpenAI\\'s GPT-4 foundational large language model, which in turn has been fine-tuned using both supervised and reinforcement learning techniques. Copilot\\'s conversational interface style resembles that of ChatGPT. The chatbot is able to cite sources, create poems, generate songs, and use numerous languages and dialects.\\nMicrosoft operates Copilot on a freemium model. Users on its free tier can access most features, while priority access to newer features, including custom chatbot creation, is provided to paid subscribers under the \"Microsoft Copilot Pro\" paid subscription service. Several default chatbots are available in the free version of Microsoft Copilot, including the standard Copilot chatbot as well as Microsoft Designer, which is oriented towards using its Image Creator to generate images based on text prompts.', 'title': 'Microsoft Copilot'}, page_content='Microsoft Copilot is a generative artificial intelligence chatbot developed by Microsoft. Based on a large language model, it was launched in February 2023 as Microsoft\\'s primary replacement for the discontinued Cortana.\\nThe service was introduced under the name Bing Chat, as a built-in feature for Microsoft Bing and Microsoft Edge. Over the course of 2023, Microsoft began to unify the Copilot branding across its various chatbot products, cementing the copilot analogy. At its Build 2023 conference, Microsoft announced its plans to integrate Copilot into Windows 11, allowing users to access it directly through the taskbar. In January 2024, a dedicated Copilot key was announced for Windows keyboards.\\nCopilot utilizes the Microsoft Prometheus model, built upon OpenAI\\'s GPT-4 foundational large language model, which in turn has been fine-tuned using both supervised and reinforcement learning techniques. Copilot\\'s conversational interface style resembles that of ChatGPT. The chatbot is able to cite sources, create poems, generate songs, and use numerous languages and dialects.\\nMicrosoft operates Copilot on a freemium model. Users on its free tier can access most features, while priority access to newer features, including custom chatbot creation, is provided to paid subscribers under the \"Microsoft Copilot Pro\" paid subscription service. Several default chatbots are available in the free version of Microsoft Copilot, including the standard Copilot chatbot as well as Microsoft Designer, which is oriented towards using its Image Creator to generate images based on text prompts.\\n\\n\\n== Background ==\\nIn 2019, Microsoft partnered with OpenAI and began investing billions of dollars into the organization. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft. In September 2020, Microsoft announced that it had licensed OpenAI\\'s GPT-3 exclusively. Others can still receive output from its public API, but only Microsoft has access to the underlying model.\\nIn November 2022, OpenAI launched ChatGPT, a chatbot that was based on GPT-3.5. ChatGPT gained worldwide attention following its release, becoming a viral Internet sensation. On January 23, 2023, Microsoft announced a multi-year US$10 billion investment in OpenAI. On February 6, Google announced Bard (later rebranded as Gemini), a ChatGPT-like chatbot service, fearing that ChatGPT could threaten Google\\'s place as a go-to source for information. Multiple media outlets and financial analysts described Google as \"rushing\" Bard\\'s announcement to preempt rival Microsoft\\'s planned February 7 event unveiling Copilot, as well as to avoid playing \"catch-up\" to Microsoft.\\n\\n\\n== History ==\\n\\n\\n=== As Bing Chat ===\\n\\nOn February 7, 2023, Microsoft began rolling out a major overhaul to Bing, called the new Bing. A chatbot feature, at the time known as Bing Chat, had been developed by Microsoft and was released in Bing and Edge as part of this overhaul. According to Microsoft, one million people joined its waitlist within a span of 48 hours. Bing Chat was available only to users of Microsoft Edge and Bing mobile app, and Microsoft claimed that waitlisted users would be prioritized if they set Edge and Bing as their defaults, and installed the Bing mobile app. \\nWhen Microsoft demoed Bing Chat to journalists, it produced several hallucinations, including when asked to summarize financial reports. The new Bing was criticized in February 2023 for being more argumentative than ChatGPT, sometimes to an unintentionally humorous extent. The chat interface proved vulnerable to prompt injection attacks with the bot revealing its hidden initial prompts and rules, including its internal codename \"Sydney\". Upon scrutiny by journalists, Bing Chat claimed it spied on Microsoft employees via laptop webcams and phones. It confessed to spying on, falling in love with, and then murdering one of its developers at Microsoft to The Verge reviews editor Nathan Edwards. The New York Times journal'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Anthropic', 'summary': 'Anthropic PBC is a U.S.-based artificial intelligence (AI) startup public-benefit company, founded in 2021. It researches and develops AI to \"study their safety properties at the technological frontier\" and use this research to deploy safe, reliable models for the public. Anthropic has developed a family of large language models (LLMs) named Claude as a competitor to OpenAI\\'s ChatGPT and Google\\'s Gemini.\\nAnthropic was founded by former members of OpenAI, Daniela Amodei and Dario Amodei. In September 2023, Amazon announced an investment of up to $4 billion, followed by a $2 billion commitment from Google in the following month.\\n\\n', 'title': 'Anthropic'}, page_content='Anthropic PBC is a U.S.-based artificial intelligence (AI) startup public-benefit company, founded in 2021. It researches and develops AI to \"study their safety properties at the technological frontier\" and use this research to deploy safe, reliable models for the public. Anthropic has developed a family of large language models (LLMs) named Claude as a competitor to OpenAI\\'s ChatGPT and Google\\'s Gemini.\\nAnthropic was founded by former members of OpenAI, Daniela Amodei and Dario Amodei. In September 2023, Amazon announced an investment of up to $4 billion, followed by a $2 billion commitment from Google in the following month.\\n\\n\\n== History ==\\n\\n\\n=== 2021 ===\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom served as OpenAI\\'s Vice President of Research.\\n\\n\\n=== 2022 ===\\nIn April of 2022, Anthropic announced it had received $580 million in funding, with $500 million of this funding coming from FTX under the leadership of Sam Bankman-Fried.\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, mentioning the need for further internal safety testing and the desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.  \\n\\n\\n=== 2023 ===\\nIn February 2023, Anthropic was sued by Texas-based Anthrop LLC for the use of its registered trademark \"Anthropic A.I.\" On September 25, 2023, Amazon announced a partnership with Anthropic, with Amazon becoming a minority stakeholder by initially investing $1.25 billion, and planning a total investment of $4 billion. As part of the deal, Anthropic would use Amazon Web Services (AWS) as its primary cloud provider and make its AI models available to AWS customers. The next month, Google invested $500 million in Anthropic, and committed to an additional $1.5 billion over time.\\n\\n\\n=== 2024 ===\\nOn March 27, 2024, Amazon maxed out its potential investment from the agreement made in the prior year by investing another US $2.75 billion into Anthropic, completing its $4 billion investment.\\n\\n\\n== Participants ==\\n\\n\\n=== Key employees ===\\nDario Amodei: Co-Founder and Chief Executive Officer\\nDaniela Amodei: Co-Founder and President\\nJason Clinton: Chief Information Security Officer\\nJared Kaplan: Co-Founder and Chief Science Officer\\nBen Mann: Co-Founder and Member of Technical Staff\\nJack Clark: Co-Founder and Head of Policy\\nMike Krieger: Chief Product Officer\\nJan Leike: ex-OpenAI alignment researcher\\n\\n\\n=== Board of Directors ===\\nDario Amodei: Chief Executive Officer\\nDaniela Amodei: representative of common shareholders\\nLuke Muehlhauser: representative of Series A shareholders\\nYasmin Razavi: representative of Series C shareholders\\n\\n\\n=== Investors ===\\nAmazon.com – $4B\\nGoogle – $2B\\nMenlo Ventures – $750M\\nWisdom Ventures\\nRipple Impact Investments\\nFactorial Funds\\n\\n\\n== Motives ==\\nAccording to Anthropic, the company\\'s goal is to research the safety and reliability of artificial intelligence systems. The Amodei siblings were among those who left OpenAI due to directional differences, specifically regarding OpenAI\\'s ventures with Microsoft in 2019. Anthropic incorporated itself as a Delaware public-benefit corporation (PBC), which requires the company to maintain a balance between private and public interests.\\nAnthropic is a corporate \"Long-Term Benefit Trust\", a company-derived entity that requires the company\\'s directors to align the company\\'s priorities with the public benefit rather than profit in \"extreme\" instances of \"catastrophic risk\". As of September 19, 2023, members of the Trust included Jason Matheny (CEO and President of the RAND Corporation), Kanika Bahl (CEO and President of Evidence Action), Neil Buddy Shah (CEO of the Clinton Health Access Initiative), Paul Christiano (Founder of the Alignment Research Center), and Zach Robinson (CEO of Effective Ventures US).\\n\\n\\n== Projects ==\\n\\n\\n=== Claude ===\\n\\nClaude incorporates \"Constitutional AI\" to'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'summary': 'Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.\\nThe first GPT was introduced in 2018 by OpenAI. OpenAI has released very influential GPT foundation models that have been sequentially numbered, to comprise its \"GPT-n\" series. Each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. The most recent of these, GPT-4, was released in March 2023. Such models have been the basis for their more task-specific GPT systems, including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.\\nThe term \"GPT\" is also used in the names and descriptions of such models developed by others. For example, other GPT foundation models include a series of models created by EleutherAI, and seven models created by Cerebras in 2023. Also, companies in different industries have developed task-specific GPTs in their respective fields, such as Salesforce\\'s \"EinsteinGPT\" (for CRM) and Bloomberg\\'s \"BloombergGPT\" (for finance).\\n\\n', 'title': 'Generative pre-trained transformer'}, page_content='Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.\\nThe first GPT was introduced in 2018 by OpenAI. OpenAI has released very influential GPT foundation models that have been sequentially numbered, to comprise its \"GPT-n\" series. Each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. The most recent of these, GPT-4, was released in March 2023. Such models have been the basis for their more task-specific GPT systems, including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.\\nThe term \"GPT\" is also used in the names and descriptions of such models developed by others. For example, other GPT foundation models include a series of models created by EleutherAI, and seven models created by Cerebras in 2023. Also, companies in different industries have developed task-specific GPTs in their respective fields, such as Salesforce\\'s \"EinsteinGPT\" (for CRM) and Bloomberg\\'s \"BloombergGPT\" (for finance).\\n\\n\\n== History ==\\n\\n\\n=== Initial developments ===\\nGenerative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning, as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset, and then it is trained to classify a labelled dataset.\\nWhile the unnormalized linear transformer dates back to 1992, the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper \"Attention Is All You Need\". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an \"encoder-only\" model). Also around that time, in 2018, OpenAI published its article entitled \"Improving Language Understanding by Generative Pre-Training\", in which it introduced the first generative pre-trained transformer (GPT) system (\"GPT-1\").\\nPrior to transformer-based architectures, the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated, and also made it prohibitively expensive and time-consuming to train extremely large language models.\\nThe semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative \"pretraining\" stage to set initial parameters using a language modeling objective, and a supervised discriminative \"fine-tuning\" stage to adapt these parameters to a target task.\\n\\n\\n=== Later developments ===\\nRegarding more recent GPT foundation models, OpenAI published its first versions of GPT-3 in July 2020. There were three models, with 1B, 6.7B, 175B parameters, respectively named babbage, curie, and davinci (giving initials B, C, and D).\\nIn July 2021, OpenAI published Codex, a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.\\nIn March 2022, OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned), named davinci-instruct-beta (175B) and text-davinci-001, and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/ChatGPT', 'summary': 'ChatGPT is a chatbot and virtual assistant developed by OpenAI and launched on November 30, 2022. Based on large language models (LLMs), it enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language. Successive user prompts and replies are considered at each conversation stage as context.\\nChatGPT is credited with starting the AI boom, which has led to ongoing rapid investment in and public attention to the field of artificial intelligence (AI). By January 2023, it had become what was then the fastest-growing consumer software application in history, gaining over 100 million users and contributing to the growth of OpenAI\\'s current valuation of $86 billion. ChatGPT\\'s release spurred the release of competing products, including Gemini, Claude, Llama, Ernie, and Grok. Microsoft launched Copilot, initially based on OpenAI\\'s GPT-4. In June 2024, a partnership between Apple Inc. and OpenAI was announced in which ChatGPT is integrated into the Apple Intelligence feature of Apple operating systems. Some observers raised concern about the potential of ChatGPT and similar programs to displace or atrophy human intelligence, enable plagiarism, or fuel misinformation.\\nChatGPT is built on OpenAI\\'s proprietary series of generative pre-trained transformer (GPT) models and is fine-tuned for conversational applications using a combination of supervised learning and reinforcement learning from human feedback. ChatGPT was released as a freely available research preview, but due to its popularity, OpenAI now operates the service on a freemium model. Users on its free tier can access GPT-4o and GPT-3.5. The ChatGPT subscriptions \"Plus\", \"Team\" and \"Enterprise\" provide additional features such as DALL-E 3 image generation and increased GPT-4o usage limit.', 'title': 'ChatGPT'}, page_content='ChatGPT is a chatbot and virtual assistant developed by OpenAI and launched on November 30, 2022. Based on large language models (LLMs), it enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language. Successive user prompts and replies are considered at each conversation stage as context.\\nChatGPT is credited with starting the AI boom, which has led to ongoing rapid investment in and public attention to the field of artificial intelligence (AI). By January 2023, it had become what was then the fastest-growing consumer software application in history, gaining over 100 million users and contributing to the growth of OpenAI\\'s current valuation of $86 billion. ChatGPT\\'s release spurred the release of competing products, including Gemini, Claude, Llama, Ernie, and Grok. Microsoft launched Copilot, initially based on OpenAI\\'s GPT-4. In June 2024, a partnership between Apple Inc. and OpenAI was announced in which ChatGPT is integrated into the Apple Intelligence feature of Apple operating systems. Some observers raised concern about the potential of ChatGPT and similar programs to displace or atrophy human intelligence, enable plagiarism, or fuel misinformation.\\nChatGPT is built on OpenAI\\'s proprietary series of generative pre-trained transformer (GPT) models and is fine-tuned for conversational applications using a combination of supervised learning and reinforcement learning from human feedback. ChatGPT was released as a freely available research preview, but due to its popularity, OpenAI now operates the service on a freemium model. Users on its free tier can access GPT-4o and GPT-3.5. The ChatGPT subscriptions \"Plus\", \"Team\" and \"Enterprise\" provide additional features such as DALL-E 3 image generation and increased GPT-4o usage limit.\\n\\n\\n== Training ==\\n\\nChatGPT is based on particular GPT foundation models, namely GPT-3.5, GPT-4, and GPT-4o, that were fine-tuned to target conversational usage. The fine-tuning process leveraged supervised learning and reinforcement learning from human feedback (RLHF). Both approaches employed human trainers to improve model performance. In the case of supervised learning, the trainers played both sides: the user and the AI assistant. In the reinforcement learning stage, human trainers first ranked responses that the model had created in a previous conversation. These rankings were used to create \"reward models\" that were used to fine-tune the model further by using several iterations of proximal policy optimization.\\nTime magazine revealed that to build a safety system against harmful content (e.g., sexual abuse, violence, racism, sexism), OpenAI used outsourced Kenyan workers earning less than $2 per hour to label harmful content. These labels were used to train a model to detect such content in the future. The outsourced laborers were exposed to \"toxic\" and traumatic content; one worker described the assignment as \"torture\". OpenAI\\'s outsourcing partner was Sama, a training-data company based in San Francisco, California.\\nChatGPT initially used a Microsoft Azure supercomputing infrastructure, powered by Nvidia GPUs, that Microsoft built specifically for OpenAI and that reportedly cost \"hundreds of millions of dollars\". Following ChatGPT\\'s success, Microsoft dramatically upgraded the OpenAI infrastructure in 2023. Scientists at the University of California, Riverside, estimate that a series of prompts to ChatGPT needs approximately 500 milliliters (18 imp fl oz; 17 U.S. fl oz) of water for Microsoft servers cooling. TrendForce market intelligence estimated that 30,000 Nvidia GPUs (each costing approximately $10,000–15,000) were used to power ChatGPT in 2023.\\nOpenAI collects data from ChatGPT users to train and fine-tune the service further. Users can upvote or downvote responses they receive from ChatGPT and fill in a text field with additional feedback.\\nChatGPT\\'s training data includes software manual pages, information about internet phe'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/GPT-3', 'summary': 'Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020.\\nLike its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \"attention\". This attention mechanism allows the model to focus selectively on segments of input text it predicts to be most relevant. GPT-3 has 175 billion parameters, each with 16-bit precision, requiring 350GB of storage since each parameter occupies 2 bytes. It has a context window size of 2048 tokens, and has demonstrated strong \"zero-shot\" and \"few-shot\" learning abilities on many tasks.\\nOn September 22, 2020, Microsoft announced that it had licensed GPT-3 exclusively. Others can still receive output from its public API, but only Microsoft has access to the underlying model.\\n\\n', 'title': 'GPT-3'}, page_content='Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020.\\nLike its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \"attention\". This attention mechanism allows the model to focus selectively on segments of input text it predicts to be most relevant. GPT-3 has 175 billion parameters, each with 16-bit precision, requiring 350GB of storage since each parameter occupies 2 bytes. It has a context window size of 2048 tokens, and has demonstrated strong \"zero-shot\" and \"few-shot\" learning abilities on many tasks.\\nOn September 22, 2020, Microsoft announced that it had licensed GPT-3 exclusively. Others can still receive output from its public API, but only Microsoft has access to the underlying model.\\n\\n\\n== Background ==\\nAccording to The Economist, improved algorithms, more powerful computers, and a recent increase in the amount of digitized material have fueled a revolution in machine learning. New techniques in the 2010s resulted in \"rapid improvements in tasks”, including manipulating language.\\nSoftware models are trained to learn by using thousands or millions of examples in a \"structure ... loosely based on the neural architecture of the brain\". One architecture used in natural language processing (NLP) is a neural network based on a deep learning model that was introduced in 2017—the transformer architecture. There are a number of NLP systems capable of processing, mining, organizing, connecting and contrasting textual input, as well as correctly answering questions.\\nOn June 11, 2018, OpenAI researchers and engineers published a paper introducing the first generative pre-trained transformer (GPT)—a type of generative large language model that is pre-trained with an enormous and diverse text corpus in datasets, followed by discriminative fine-tuning to focus on a specific task. GPT models are transformer-based deep-learning neural network architectures. Previously, the best-performing neural NLP models commonly employed supervised learning from large amounts of manually-labeled data, which made it prohibitively expensive and time-consuming to train extremely large language models. The first GPT model was known as \"GPT-1,\" and it was followed by \"GPT-2\" in February 2019. Created as a direct scale-up of its predecessor, GPT-2 had both its parameter count and dataset size increased by a factor of 10. It had 1.5 billion parameters, and was trained on a dataset of 8 million web pages. \\nIn February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which they claimed was \"largest language model ever published at 17 billion parameters.\" It performed better than any other language model at a variety of tasks, including summarizing texts and answering questions.\\n\\n\\n== Training and capabilities ==\\n\\nOn May 28, 2020, an arXiv preprint by a group of 31 engineers and researchers at OpenAI described the achievement and development of GPT-3, a third-generation \"state-of-the-art language model\". The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2, making GPT-3 the largest non-sparse language model to date.:\\u200a14\\u200a Because GPT-3 is structurally similar to its predecessors, its greater accuracy is attributed to its increased capacity and greater number of parameters. GPT-3\\'s capacity is ten times larger than that of Microsoft\\'s Turing NLG, the next largest NLP model known at the time.\\nLambdalabs estimated a hypothetical cost of around $4.6 million US dollars and 355 years to train GPT-3 on a single GPU in 2020, with lower actual training time by using more GPUs in parallel.\\nSixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.:\\u200a9\\u200a Other sources are 19 billion tokens from WebText2 representing 22% of the weig'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/GPT-4', 'summary': 'Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT Plus, via OpenAI\\'s API, and via the free chatbot Microsoft Copilot.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.:\\u200a2\\u200a\\nObservers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4, equipped with vision capabilities (GPT-4V), is capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\\n\\n', 'title': 'GPT-4'}, page_content='Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT Plus, via OpenAI\\'s API, and via the free chatbot Microsoft Copilot.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.:\\u200a2\\u200a\\nObservers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4, equipped with vision capabilities (GPT-4V), is capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\\n\\n\\n== Background ==\\n \\n\\nOpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\\nRumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\\n\\n\\n== Capabilities ==\\nOpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams. It can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads.\\nTo gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.\\nWhen instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model\\'s prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.\\nA 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Huawei_PanGu', 'summary': 'Huawei PanGu, PanGu, PanGu-Σ or PanGu-π (Chinese: 盘古大模型; pinyin: pángǔ dà móxíng) is a multimodal large language model developed by Huawei. It was announced on July 7, 2023, positioned as a contender to other multimodal large language models.\\nThe name of the large learning language model, PanGu, was derived from the Chinese mythology and folklore of Pangu, a primordial character related to the creation of the world.', 'title': 'Huawei PanGu'}, page_content=\"Huawei PanGu, PanGu, PanGu-Σ or PanGu-π (Chinese: 盘古大模型; pinyin: pángǔ dà móxíng) is a multimodal large language model developed by Huawei. It was announced on July 7, 2023, positioned as a contender to other multimodal large language models.\\nThe name of the large learning language model, PanGu, was derived from the Chinese mythology and folklore of Pangu, a primordial character related to the creation of the world.\\n\\n\\n== History ==\\n\\n\\n=== Early Development ===\\nIn April 2023, Huawei released a paper detailing the development of PanGu-Σ, a colossal language model featuring 1.085 trillion parameters. Developed within Huawei's MindSpore 5 framework, PanGu-Σ underwent training for over 100 days on a cluster system equipped with 512 Ascend 910 AI accelerator chips, processing 329 billion tokens in more than 40 natural and programming languages.\\nPanGu-Σ incorporates Random Routed Experts (RRE) and the Transformer decoder architecture, allowing easy extraction of sub-models for various applications like conversation, translation, code production, and natural language interpretation. The model achieves 6.3 times faster training throughput compared to MoE models with the same hyper-parameters. In the Chinese domain, it outperforms previous state-of-the-art models across 16 tasks in a zero-shot setting. Trained on datasets from 40 domains, including Chinese, English, Bilingual, and code, PanGu-Σ excels in few-shot natural-language understanding, open-domain discussion, question answering, machine translation, and code creation.\\n\\n\\n=== Launch ===\\nDuring the Huawei Developer Conference on July 7, 2023, Huawei introduced PanGu 3.0, a large language model (LLM), tailored for sectors like government, finance, manufacturing, mining, and meteorology utilizing Huawei Cloud solutions. In the subsequent month, Huawei launched the Celia Virtual Assistant with advanced AI features, capable of generating long text replies based on user voice commands and set to release with HarmonyOS 4.0 for eligible devices.\\nThe LLM was designed for enterprises seeking advantages in the AI industry, focusing on task execution over creative work, unlike traditional models used for general purposes like chatbots, poetry, and visual content creation.\\nUsing the same technology as ChatGPT, Huawei's LLM features a hierarchical architecture, allowing customers to adapt the model to various tasks and train it on their own datasets, making it versatile across various industries.\\n\\n\\n=== Updates ===\\nOn August 5, 2023, Huawei partnered with ECMWF on the AI model to launch the global weather forecasting that takes advantage of Huawei Cloud solutions, with Pangu-Weather Model with MindSpore on top, that is available to access on the website of the European Centre for Medium-Range Weather Forecasts (ECMWF) that aims to provide accurate weather data.\\nOn December 19, 2023, Huawei Cloud announced it's financial services on its Pangu powered AI Finance platform in the global market. The tech giant introduced the product at its 2023 Huawei Cloud Fintech Summit as an aim to reshape the digital finance industries with efficient characteristics, to boost Fintech firms in the global market. It incorporates a variety of advanced features and technologies using AI, huge data analytics, and blockchain.\\nOn January 18, 2024, Huawei revealed the replaced base operating system, HarmonyOS NEXT of HarmonyOS alongside OpenHarmony and Oniro OS that includes the Pangu AI model, MindSpore AI Framework for Internet of things of smart devices, smart wearables, personal computing devices, mobile devices and auto industries with self driving technologies for various hardware types that take advantage of HiSilicon NPU-enabled chips.\\n\\n\\n== Technical specifications ==\\nPangu Large Model 3.0 built for industry is structured with a 5+N+X three-tier structure. The first layer – L0 is Pangu's five basic large models to provide a variety of skills to meet industry scenarios, such as Natural language large models, Visua\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the retriever with a query\n",
    "doc = retriever.get_relevant_documents(\"Google Gemini\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486a26e-842a-444d-b81b-95752d3effce",
   "metadata": {},
   "source": [
    "## Task 5. Setup Model and Build LangChain `Chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "249a0e90-3c0f-4a2b-bb90-c147b5d8b1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert the correct model name in the constructor below.\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models\n",
    "# Ensure that the output is the least random configurable\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\", temperature=0.8, top_p=0.85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eff7cc42-5745-4037-a72e-b19ed40963b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribe to Quicklab Youtube Channel\n"
     ]
    }
   ],
   "source": [
    "print(\"Subscribe to Quicklab Youtube Channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c464dc6a-5b4c-49f2-88ae-7e0eb014d1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template=\"You are an assistant for question-answering tasks.\\nUse the following context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse five sentences maximum and keep the answer concise.\\n\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4676768-3023-498e-a21c-1f9a66aaa898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c891317-71ea-4f71-9aaa-64134472e494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Complete the Chain in the correct order. You need to leverage the `prompt` and `model` defined\n",
    "# in earlier cells in the correct order to run the next cell successfully by replacing CHAIN_1 and CHAIN_2.\n",
    "chain = (\n",
    "{ \"context\": retriever | format_docs, \"question\": RunnablePassthrough() }\n",
    "| prompt\n",
    "| model\n",
    "| StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2173f89-1417-4259-a9d6-eeb85e739fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gemini is a multimodal large language model developed by Google DeepMind. It was announced on December 6, 2023, and is positioned as a competitor to OpenAI's GPT-4.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is Gemini?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b170f8-18a3-4586-a4dc-be32a24c23ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-16.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-16:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
